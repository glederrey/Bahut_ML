{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jour 3 - Exercice 1 : Classification de la Satisfaction des Passagers\n",
    "\n",
    "## Objectifs\n",
    "- Appliquer les techniques de machine learning à un problème de classification réel\n",
    "- Utiliser les données de satisfaction des passagers pour prédire leur niveau de satisfaction\n",
    "- Évaluer les performances des modèles sur un jeu de test\n",
    "- Visualiser et interpréter les résultats avec Plotly\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Dans ce notebook, nous allons travailler sur un problème de classification binaire : prédire si un passager est satisfait ou non de son expérience de vol. Nous utiliserons les données de satisfaction des passagers et appliquerons différentes techniques de machine learning pour construire et évaluer des modèles de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Visualisation avec Plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Scikit-learn pour le machine learning\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "# Modèles de classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Ajouter le chemin pour importer utils.py\n",
    "sys.path.append(os.path.abspath('./'))\n",
    "from utils import results_predictions_satisfaction\n",
    "\n",
    "# Pour afficher plus de colonnes dans les DataFrames\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données d'entraînement\n",
    "train_df = pd.read_csv('../../data/passenger_satisfaction/train.csv')\n",
    "\n",
    "# Affichage des premières lignes\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations sur le DataFrame\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des valeurs manquantes\n",
    "missing_values = train_df.isnull().sum()\n",
    "missing_values_pct = (missing_values / len(train_df)) * 100\n",
    "\n",
    "# Création d'un DataFrame pour visualiser les valeurs manquantes\n",
    "missing_df = pd.DataFrame({\n",
    "    'Nombre de valeurs manquantes': missing_values,\n",
    "    'Pourcentage (%)': missing_values_pct\n",
    "})\n",
    "\n",
    "# Affichage des colonnes avec des valeurs manquantes\n",
    "missing_df[missing_df['Nombre de valeurs manquantes'] > 0].sort_values('Nombre de valeurs manquantes', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualisation des données\n",
    "\n",
    "Explorons les données pour mieux comprendre la distribution des variables et leurs relations avec la satisfaction des passagers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de la variable cible\n",
    "satisfaction_counts = train_df['Satisfaction'].value_counts().reset_index()\n",
    "satisfaction_counts.columns = ['Satisfaction', 'Count']\n",
    "\n",
    "fig = px.pie(satisfaction_counts, values='Count', names='Satisfaction', \n",
    "             title='Distribution de la satisfaction des passagers',\n",
    "             color_discrete_sequence=px.colors.qualitative.Set2)\n",
    "fig.update_traces(textinfo='percent+label')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation entre l'âge et la satisfaction\n",
    "fig = px.histogram(train_df, x='Age', color='Satisfaction', \n",
    "                  title='Distribution de l\\'âge par niveau de satisfaction',\n",
    "                  barmode='group', nbins=20,\n",
    "                  color_discrete_sequence=px.colors.qualitative.Set2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation entre la classe et la satisfaction\n",
    "class_satisfaction = pd.crosstab(train_df['Class'], train_df['Satisfaction'])\n",
    "class_satisfaction_pct = class_satisfaction.div(class_satisfaction.sum(axis=1), axis=0) * 100\n",
    "\n",
    "fig = px.bar(class_satisfaction_pct.reset_index().melt(id_vars='Class', var_name='Satisfaction', value_name='Percentage'),\n",
    "            x='Class', y='Percentage', color='Satisfaction',\n",
    "            title='Pourcentage de satisfaction par classe',\n",
    "            color_discrete_sequence=px.colors.qualitative.Set2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation entre le type de voyage et la satisfaction\n",
    "travel_satisfaction = pd.crosstab(train_df['Type of Travel'], train_df['Satisfaction'])\n",
    "travel_satisfaction_pct = travel_satisfaction.div(travel_satisfaction.sum(axis=1), axis=0) * 100\n",
    "\n",
    "fig = px.bar(travel_satisfaction_pct.reset_index().melt(id_vars='Type of Travel', var_name='Satisfaction', value_name='Percentage'),\n",
    "            x='Type of Travel', y='Percentage', color='Satisfaction',\n",
    "            title='Pourcentage de satisfaction par type de voyage',\n",
    "            color_discrete_sequence=px.colors.qualitative.Set2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des scores de satisfaction pour différents services\n",
    "service_columns = ['Inflight wifi service', 'Departure/Arrival time convenient', 'Ease of Online booking',\n",
    "                   'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort',\n",
    "                   'Inflight entertainment', 'On-board service', 'Leg room service',\n",
    "                   'Baggage handling', 'Checkin service', 'Inflight service', 'Cleanliness']\n",
    "\n",
    "# Créer un DataFrame pour la visualisation\n",
    "service_data = []\n",
    "for col in service_columns:\n",
    "    for satisfaction in train_df['Satisfaction'].unique():\n",
    "        subset = train_df[train_df['Satisfaction'] == satisfaction]\n",
    "        for rating in range(6):  # 0-5 ratings\n",
    "            count = (subset[col] == rating).sum()\n",
    "            service_data.append({\n",
    "                'Service': col,\n",
    "                'Rating': rating,\n",
    "                'Satisfaction': satisfaction,\n",
    "                'Count': count\n",
    "            })\n",
    "\n",
    "service_df = pd.DataFrame(service_data)\n",
    "\n",
    "# Créer un subplot pour chaque service\n",
    "fig = make_subplots(rows=7, cols=2, subplot_titles=service_columns, vertical_spacing=0.1)\n",
    "\n",
    "row, col = 1, 1\n",
    "for service in service_columns:\n",
    "    service_subset = service_df[service_df['Service'] == service]\n",
    "    \n",
    "    for i, satisfaction in enumerate(train_df['Satisfaction'].unique()):\n",
    "        satisfaction_subset = service_subset[service_subset['Satisfaction'] == satisfaction]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=satisfaction_subset['Rating'],\n",
    "                y=satisfaction_subset['Count'],\n",
    "                name=satisfaction if row == 1 and col == 1 else None,\n",
    "                showlegend=(row == 1 and col == 1),\n",
    "                marker_color=px.colors.qualitative.Set2[i]\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "    \n",
    "    col += 1\n",
    "    if col > 2:\n",
    "        col = 1\n",
    "        row += 1\n",
    "\n",
    "fig.update_layout(height=1800, width=1000, title_text=\"Distribution des évaluations de service par niveau de satisfaction\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Préparation des données pour le modèle\n",
    "\n",
    "Nous allons maintenant préparer les données pour l'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la colonne ID qui n'est pas utile pour la prédiction\n",
    "train_df = train_df.drop('ID', axis=1)\n",
    "\n",
    "# Séparation des features et de la variable cible\n",
    "X = train_df.drop('Satisfaction', axis=1)\n",
    "y = train_df['Satisfaction']\n",
    "\n",
    "# Séparation des variables numériques et catégorielles\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Variables numériques: {len(numeric_features)}\")\n",
    "print(f\"Variables catégorielles: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un pipeline de prétraitement\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Séparation des données d'entraînement et de validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entraînement et évaluation des modèles\n",
    "\n",
    "Nous allons maintenant entraîner différents modèles de classification et évaluer leurs performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des modèles à tester\n",
    "models = {\n",
    "    'Régression Logistique': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Dictionnaire pour stocker les résultats\n",
    "results = {}\n",
    "\n",
    "# Entraînement et évaluation de chaque modèle\n",
    "for name, model in models.items():\n",
    "    print(f\"Entraînement du modèle: {name}\")\n",
    "    \n",
    "    # Création du pipeline complet\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', model)])\n",
    "    \n",
    "    # Entraînement du modèle\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Prédictions sur l'ensemble de validation\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, pos_label='satisfied')\n",
    "    \n",
    "    # Stockage des résultats\n",
    "    results[name] = {\n",
    "        'pipeline': pipeline,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(\"\\nRapport de classification:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des résultats\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[name]['accuracy'] for name in model_names]\n",
    "f1_scores = [results[name]['f1_score'] for name in model_names]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Accuracy', x=model_names, y=accuracies, marker_color='royalblue'),\n",
    "    go.Bar(name='F1-score', x=model_names, y=f1_scores, marker_color='lightcoral')\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Comparaison des performances des modèles',\n",
    "    xaxis_title='Modèle',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    yaxis=dict(range=[0, 1])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Évaluation sur le jeu de test\n",
    "\n",
    "Maintenant, nous allons évaluer notre meilleur modèle sur le jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données de test\n",
    "test_df = pd.read_csv('../../data/passenger_satisfaction/test.csv')\n",
    "test_df = test_df.drop('ID', axis=1)\n",
    "\n",
    "# Affichage des premières lignes\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection du meilleur modèle (celui avec le meilleur F1-score)\n",
    "best_model_name = max(results, key=lambda x: results[x]['f1_score'])\n",
    "best_pipeline = results[best_model_name]['pipeline']\n",
    "\n",
    "print(f\"Meilleur modèle: {best_model_name}\")\n",
    "print(f\"F1-score sur la validation: {results[best_model_name]['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions sur le jeu de test\n",
    "test_predictions = best_pipeline.predict(test_df)\n",
    "\n",
    "# Affichage des premières prédictions\n",
    "pd.DataFrame({\n",
    "    'Prédiction': test_predictions\n",
    "}).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation des prédictions avec la fonction results_predictions_satisfaction\n",
    "test_f1_score = results_predictions_satisfaction(test_predictions)\n",
    "print(f\"F1-score sur le jeu de test: {test_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse des caractéristiques importantes (pour Random Forest)\n",
    "\n",
    "Si nous avons utilisé un modèle Random Forest, nous pouvons analyser l'importance des caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification si le meilleur modèle est un Random Forest\n",
    "if best_model_name == 'Random Forest':\n",
    "    # Extraction du modèle Random Forest du pipeline\n",
    "    rf_model = best_pipeline.named_steps['classifier']\n",
    "    \n",
    "    # Extraction des noms des caractéristiques après prétraitement\n",
    "    preprocessor = best_pipeline.named_steps['preprocessor']\n",
    "    cat_features = preprocessor.transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "    feature_names = numeric_features + list(cat_features)\n",
    "    \n",
    "    # Extraction des importances des caractéristiques\n",
    "    importances = rf_model.feature_importances_\n",
    "    \n",
    "    # Création d'un DataFrame pour la visualisation\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False).head(20)\n",
    "    \n",
    "    # Visualisation\n",
    "    fig = px.bar(feature_importance_df, x='Importance', y='Feature', \n",
    "                orientation='h', title='Top 20 des caractéristiques les plus importantes',\n",
    "                color='Importance', color_continuous_scale='Viridis')\n",
    "    fig.show()\n",
    "else:\n",
    "    print(f\"Le meilleur modèle est {best_model_name}, qui ne fournit pas d'importance des caractéristiques.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons:\n",
    "1. Exploré et visualisé les données de satisfaction des passagers\n",
    "2. Préparé les données pour l'entraînement des modèles\n",
    "3. Entraîné et évalué plusieurs modèles de classification\n",
    "4. Sélectionné le meilleur modèle et évalué ses performances sur le jeu de test\n",
    "5. Analysé les caractéristiques les plus importantes (si applicable)\n",
    "\n",
    "Le modèle final a obtenu un F1-score de [valeur] sur le jeu de test, ce qui indique [interprétation des performances].\n",
    "\n",
    "### Pistes d'amélioration\n",
    "- Essayer d'autres algorithmes de classification (XGBoost, LightGBM, etc.)\n",
    "- Optimiser les hyperparamètres des modèles avec une recherche par grille ou une recherche aléatoire\n",
    "- Créer de nouvelles caractéristiques à partir des données existantes\n",
    "- Utiliser des techniques d'ensemble pour combiner plusieurs modèles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bahut_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
