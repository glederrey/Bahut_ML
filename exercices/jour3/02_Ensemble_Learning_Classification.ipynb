{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jour 3 - Exercice 2 : Ensemble Learning pour la Classification\n",
    "\n",
    "## Objectifs\n",
    "- Comprendre et appliquer les techniques d'Ensemble Learning pour la classification\n",
    "- Implémenter différentes méthodes d'ensemble : Bagging, Boosting et Stacking\n",
    "- Évaluer les performances des modèles sur le jeu de test\n",
    "- Visualiser et interpréter les résultats avec Plotly\n",
    "- Comparer les performances des différentes approches d'ensemble\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Dans ce notebook, nous allons explorer les techniques d'Ensemble Learning pour améliorer les performances de nos modèles de classification. L'Ensemble Learning consiste à combiner plusieurs modèles pour obtenir de meilleures prédictions que celles obtenues avec un seul modèle. Nous utiliserons les données de satisfaction des passagers et appliquerons différentes techniques d'ensemble pour prédire si un passager est satisfait ou non de son expérience de vol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des bibliothèques et chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Visualisation avec Plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Scikit-learn pour le machine learning\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, roc_curve, auc\n",
    "\n",
    "# Modèles de base\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Modèles d'ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Ajouter le chemin pour importer utils.py\n",
    "sys.path.append(os.path.abspath('./'))\n",
    "from utils import results_predictions_satisfaction\n",
    "\n",
    "# Pour afficher plus de colonnes dans les DataFrames\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données d'entraînement\n",
    "train_df = pd.read_csv('../../data/passenger_satisfaction/train.csv')\n",
    "\n",
    "# Affichage des premières lignes\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données de test\n",
    "test_df = pd.read_csv('../../data/passenger_satisfaction/test.csv')\n",
    "\n",
    "# Affichage des premières lignes\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de la distribution de la variable cible dans le jeu d'entraînement\n",
    "target_counts = train_df['Satisfaction'].value_counts()\n",
    "\n",
    "fig = px.pie(values=target_counts.values, \n",
    "             names=target_counts.index, \n",
    "             title='Distribution de la satisfaction des passagers (Train)',\n",
    "             color_discrete_sequence=px.colors.qualitative.Set3)\n",
    "\n",
    "fig.update_traces(textinfo='percent+label')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Préparation des données\n",
    "\n",
    "Nous allons préparer nos données pour l'entraînement des modèles d'ensemble. Cela comprend le nettoyage des données, la gestion des valeurs manquantes, l'encodage des variables catégorielles et la normalisation des variables numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des valeurs manquantes dans le jeu d'entraînement\n",
    "missing_values = train_df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "if len(missing_values) > 0:\n",
    "    print(\"Valeurs manquantes dans le jeu d'entraînement :\")\n",
    "    print(missing_values)\n",
    "else:\n",
    "    print(\"Aucune valeur manquante dans le jeu d'entraînement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des valeurs manquantes dans le jeu de test\n",
    "missing_values_test = test_df.isnull().sum()\n",
    "missing_values_test = missing_values_test[missing_values_test > 0]\n",
    "\n",
    "if len(missing_values_test) > 0:\n",
    "    print(\"Valeurs manquantes dans le jeu de test :\")\n",
    "    print(missing_values_test)\n",
    "else:\n",
    "    print(\"Aucune valeur manquante dans le jeu de test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de la colonne ID qui n'est pas utile pour la prédiction\n",
    "train_df = train_df.drop('ID', axis=1)\n",
    "test_df = test_df.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des features et de la variable cible pour le jeu d'entraînement\n",
    "X_train = train_df.drop('Satisfaction', axis=1)\n",
    "y_train = train_df['Satisfaction']\n",
    "\n",
    "# Préparation du jeu de test\n",
    "X_test = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification des colonnes numériques et catégorielles\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Features numériques : {numeric_features}\")\n",
    "print(f\"Features catégorielles : {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un pipeline de prétraitement\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Application du prétraitement aux données d'entraînement\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Application du même prétraitement aux données de test\n",
    "X_test_preprocessed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de la forme des données prétraitées\n",
    "print(f\"Forme de X_train_preprocessed : {X_train_preprocessed.shape}\")\n",
    "print(f\"Forme de X_test_preprocessed : {X_test_preprocessed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un jeu de validation à partir du jeu d'entraînement\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_preprocessed, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Forme de X_train_final : {X_train_final.shape}\")\n",
    "print(f\"Forme de X_val : {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implémentation des modèles d'Ensemble Learning\n",
    "\n",
    "Dans cette section, nous allons implémenter différentes techniques d'Ensemble Learning pour la classification :\n",
    "- Bagging (Random Forest, Bagging Classifier)\n",
    "- Boosting (AdaBoost, Gradient Boosting)\n",
    "- Voting (Hard et Soft Voting)\n",
    "- Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Bagging (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémentation du Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Prédictions sur le jeu de validation\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"Performances du Random Forest sur le jeu de validation :\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred_rf):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_val, y_val_pred_rf, pos_label='satisfied'):.4f}\")\n",
    "print(\"\\nRapport de classification :\")\n",
    "print(classification_report(y_val, y_val_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la matrice de confusion pour le Random Forest\n",
    "conf_matrix_rf = confusion_matrix(y_val, y_val_pred_rf)\n",
    "\n",
    "# Création de la figure avec Plotly\n",
    "fig = px.imshow(conf_matrix_rf,\n",
    "                labels=dict(x=\"Prédiction\", y=\"Réalité\", color=\"Nombre\"),\n",
    "                x=['neutral or dissatisfied', 'satisfied'],\n",
    "                y=['neutral or dissatisfied', 'satisfied'],\n",
    "                title=\"Matrice de confusion - Random Forest\",\n",
    "                color_continuous_scale='Blues')\n",
    "\n",
    "# Ajout des annotations\n",
    "for i in range(len(conf_matrix_rf)):\n",
    "    for j in range(len(conf_matrix_rf[i])):\n",
    "        fig.add_annotation(x=j, y=i, \n",
    "                          text=str(conf_matrix_rf[i, j]),\n",
    "                          showarrow=False,\n",
    "                          font=dict(color=\"white\" if conf_matrix_rf[i, j] > conf_matrix_rf.max()/2 else \"black\"))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémentation du Bagging Classifier avec un arbre de décision comme estimateur de base\n",
    "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), \n",
    "                                 n_estimators=100, \n",
    "                                 random_state=42)\n",
    "bagging_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Prédictions sur le jeu de validation\n",
    "y_val_pred_bagging = bagging_model.predict(X_val)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"Performances du Bagging Classifier sur le jeu de validation :\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred_bagging):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_val, y_val_pred_bagging, pos_label='satisfied'):.4f}\")\n",
    "print(\"\\nRapport de classification :\")\n",
    "print(classification_report(y_val, y_val_pred_bagging))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Boosting (AdaBoost, Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémentation de l'AdaBoost\n",
    "adaboost_model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), \n",
    "                                   n_estimators=100, \n",
    "                                   random_state=42)\n",
    "adaboost_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Prédictions sur le jeu de validation\n",
    "y_val_pred_adaboost = adaboost_model.predict(X_val)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"Performances de l'AdaBoost sur le jeu de validation :\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred_adaboost):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_val, y_val_pred_adaboost, pos_label='satisfied'):.4f}\")\n",
    "print(\"\\nRapport de classification :\")\n",
    "print(classification_report(y_val, y_val_pred_adaboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémentation du Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, \n",
    "                                     learning_rate=0.1, \n",
    "                                     max_depth=3, \n",
    "                                     random_state=42)\n",
    "gb_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Prédictions sur le jeu de validation\n",
    "y_val_pred_gb = gb_model.predict(X_val)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"Performances du Gradient Boosting sur le jeu de validation :\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred_gb):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_val, y_val_pred_gb, pos_label='satisfied'):.4f}\")\n",
    "print(\"\\nRapport de classification :\")\n",
    "print(classification_report(y_val, y_val_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des modèles de base pour le Voting Classifier\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Implémentation du Hard Voting\n",
    "hard_voting = VotingClassifier(\n",
    "    estimators=[('lr', log_reg), ('rf', rf), ('gb', gb)],\n",
    "    voting='hard'\n",
    ")\n",
    "hard_voting.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Prédictions sur le jeu de validation\n",
    "y_val_pred_hard = hard_voting.predict(X_val)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"Performances du Hard Voting sur le jeu de validation :\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred_hard):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_val, y_val_pred_hard, pos_label='satisfied'):.4f}\")\n",
    "print(\"\\nRapport de classification :\")\n",
    "print(classification_report(y_val, y_val_pred_hard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémentation du Soft Voting\n",
    "soft_voting = VotingClassifier(\n",
    "    estimators=[('lr', log_reg), ('rf', rf), ('gb', gb)],\n",
    "    voting='soft'\n",
    ")\n",
    "soft_voting.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Prédictions sur le jeu de validation\n",
    "y_val_pred_soft = soft_voting.predict(X_val)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"Performances du Soft Voting sur le jeu de validation :\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred_soft):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_val, y_val_pred_soft, pos_label='satisfied'):.4f}\")\n",
    "print(\"\\nRapport de classification :\")\n",
    "print(classification_report(y_val, y_val_pred_soft))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des modèles de base pour le Stacking\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
    "]\n",
    "\n",
    "# Implémentation du Stacking avec un modèle final de régression logistique\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "    cv=5\n",
    ")\n",
    "stacking_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Prédictions sur le jeu de validation\n",
    "y_val_pred_stacking = stacking_model.predict(X_val)\n",
    "\n",
    "# Évaluation des performances\n",
    "print(\"Performances du Stacking sur le jeu de validation :\")\n",
    "print(f\"Accuracy: {accuracy_score(y_val, y_val_pred_stacking):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_val, y_val_pred_stacking, pos_label='satisfied'):.4f}\")\n",
    "print(\"\\nRapport de classification :\")\n",
    "print(classification_report(y_val, y_val_pred_stacking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparaison des modèles et sélection du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataFrame pour comparer les performances des différents modèles\n",
    "models = {\n",
    "    'Random Forest': (rf_model, y_val_pred_rf),\n",
    "    'Bagging Classifier': (bagging_model, y_val_pred_bagging),\n",
    "    'AdaBoost': (adaboost_model, y_val_pred_adaboost),\n",
    "    'Gradient Boosting': (gb_model, y_val_pred_gb),\n",
    "    'Hard Voting': (hard_voting, y_val_pred_hard),\n",
    "    'Soft Voting': (soft_voting, y_val_pred_soft),\n",
    "    'Stacking': (stacking_model, y_val_pred_stacking)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for model_name, (model, y_pred) in models.items():\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, pos_label='satisfied')\n",
    "    results.append({\n",
    "        'Modèle': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='F1 Score', ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des performances des modèles\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Accuracy', 'F1 Score'))\n",
    "\n",
    "# Tri des modèles par F1 Score\n",
    "sorted_results = results_df.sort_values(by='F1 Score', ascending=True)\n",
    "\n",
    "# Ajout des barres pour l'Accuracy\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=sorted_results['Accuracy'],\n",
    "        y=sorted_results['Modèle'],\n",
    "        orientation='h',\n",
    "        marker=dict(color='royalblue'),\n",
    "        name='Accuracy'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Ajout des barres pour le F1 Score\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=sorted_results['F1 Score'],\n",
    "        y=sorted_results['Modèle'],\n",
    "        orientation='h',\n",
    "        marker=dict(color='firebrick'),\n",
    "        name='F1 Score'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Mise à jour de la mise en page\n",
    "fig.update_layout(\n",
    "    title_text=\"Comparaison des performances des modèles d'ensemble\",\n",
    "    height=500,\n",
    "    width=1000,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Évaluation du meilleur modèle sur le jeu de test\n",
    "\n",
    "Maintenant que nous avons comparé les performances des différents modèles sur le jeu de validation, nous allons sélectionner le meilleur modèle et l'évaluer sur le jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection du meilleur modèle (celui avec le F1 Score le plus élevé)\n",
    "best_model_name = results_df.iloc[0]['Modèle']\n",
    "best_model = models[best_model_name][0]\n",
    "\n",
    "print(f\"Le meilleur modèle est : {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réentraînement du meilleur modèle sur l'ensemble du jeu d'entraînement\n",
    "# (X_train_preprocessed et y_train)\n",
    "best_model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Prédictions sur le jeu de test\n",
    "y_test_pred = best_model.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation de la fonction results_predictions_satisfaction pour évaluer les performances sur le jeu de test\n",
    "f1_test = results_predictions_satisfaction(y_test_pred)\n",
    "print(f\"F1 Score sur le jeu de test : {f1_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse des caractéristiques importantes\n",
    "\n",
    "Pour certains modèles comme Random Forest et Gradient Boosting, nous pouvons extraire l'importance des caractéristiques pour comprendre quels facteurs influencent le plus la satisfaction des passagers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification si le meilleur modèle permet d'extraire l'importance des caractéristiques\n",
    "if hasattr(best_model, 'feature_importances_') or (hasattr(best_model, 'estimators_') and hasattr(best_model.estimators_[0], 'feature_importances_')):\n",
    "    # Extraction de l'importance des caractéristiques\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        feature_importances = best_model.feature_importances_\n",
    "    else:\n",
    "        # Pour les modèles d'ensemble comme VotingClassifier, on peut prendre le premier estimateur\n",
    "        for estimator_name, estimator in best_model.named_estimators_.items():\n",
    "            if hasattr(estimator, 'feature_importances_'):\n",
    "                feature_importances = estimator.feature_importances_\n",
    "                break\n",
    "    \n",
    "    # Récupération des noms des caractéristiques après prétraitement\n",
    "    # Cela peut être complexe avec ColumnTransformer, donc nous allons utiliser une approche simplifiée\n",
    "    # en utilisant les noms des colonnes originales\n",
    "    \n",
    "    # Création d'un DataFrame pour visualiser l'importance des caractéristiques\n",
    "    if 'feature_importances_' in locals():\n",
    "        # Obtention des noms de caractéristiques après one-hot encoding\n",
    "        feature_names = []\n",
    "        for name, transformer, features in preprocessor.transformers_:\n",
    "            if name == 'cat' and hasattr(transformer.named_steps['onehot'], 'get_feature_names_out'):\n",
    "                # Pour les caractéristiques catégorielles, obtenir les noms après one-hot encoding\n",
    "                cat_features = transformer.named_steps['onehot'].get_feature_names_out(features)\n",
    "                feature_names.extend(cat_features)\n",
    "            else:\n",
    "                # Pour les caractéristiques numériques, utiliser les noms d'origine\n",
    "                feature_names.extend(features)\n",
    "        \n",
    "        # Si le nombre de noms de caractéristiques ne correspond pas au nombre d'importances\n",
    "        # Utiliser des indices comme noms de caractéristiques\n",
    "        if len(feature_names) != len(feature_importances):\n",
    "            feature_names = [f\"Feature {i}\" for i in range(len(feature_importances))]\n",
    "        \n",
    "        # Création du DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': feature_importances\n",
    "        })\n",
    "        \n",
    "        # Tri par importance décroissante\n",
    "        importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        # Affichage des 15 caractéristiques les plus importantes\n",
    "        top_15 = importance_df.head(15)\n",
    "        \n",
    "        # Visualisation avec Plotly\n",
    "        fig = px.bar(top_15, \n",
    "                     x='Importance', \n",
    "                     y='Feature', \n",
    "                     orientation='h',\n",
    "                     title=f\"Top 15 des caractéristiques les plus importantes ({best_model_name})\",\n",
    "                     color='Importance',\n",
    "                     color_continuous_scale='Viridis')\n",
    "        \n",
    "        fig.update_layout(yaxis={'categoryorder':'total ascending'})\n",
    "        fig.show()\n",
    "else:\n",
    "    print(f\"Le modèle {best_model_name} ne fournit pas d'importance des caractéristiques.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Courbes ROC et AUC\n",
    "\n",
    "Nous allons maintenant visualiser les courbes ROC et calculer l'AUC pour les différents modèles afin de comparer leurs performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un encodeur pour transformer les étiquettes en valeurs numériques\n",
    "le = LabelEncoder()\n",
    "y_val_encoded = le.fit_transform(y_val)\n",
    "\n",
    "# Initialisation de la figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Couleurs pour les différents modèles\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "# Calcul des courbes ROC pour chaque modèle\n",
    "for i, (model_name, (model, _)) in enumerate(models.items()):\n",
    "    # Vérification si le modèle peut prédire des probabilités\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        # Prédiction des probabilités\n",
    "        y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Calcul de la courbe ROC\n",
    "        fpr, tpr, _ = roc_curve(y_val_encoded, y_val_proba)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        \n",
    "        # Ajout de la courbe ROC à la figure\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=fpr, y=tpr,\n",
    "            name=f\"{model_name} (AUC = {auc_score:.4f})\",\n",
    "            mode='lines',\n",
    "            line=dict(color=colors[i % len(colors)], width=2)\n",
    "        ))\n",
    "\n",
    "# Ajout de la ligne de référence (random classifier)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1], y=[0, 1],\n",
    "    name='Random Classifier',\n",
    "    mode='lines',\n",
    "    line=dict(color='black', width=2, dash='dash')\n",
    "))\n",
    "\n",
    "# Mise à jour de la mise en page\n",
    "fig.update_layout(\n",
    "    title_text=\"Courbes ROC pour les différents modèles d'ensemble\",\n",
    "    xaxis_title=\"Taux de faux positifs\",\n",
    "    yaxis_title=\"Taux de vrais positifs\",\n",
    "    legend=dict(x=0.01, y=0.99, bgcolor='rgba(255, 255, 255, 0.5)'),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons exploré différentes techniques d'Ensemble Learning pour la classification de la satisfaction des passagers. Nous avons implémenté et comparé plusieurs approches :\n",
    "\n",
    "1. **Bagging** : Random Forest et Bagging Classifier\n",
    "2. **Boosting** : AdaBoost et Gradient Boosting\n",
    "3. **Voting** : Hard Voting et Soft Voting\n",
    "4. **Stacking** : Combinaison de plusieurs modèles avec un méta-modèle\n",
    "\n",
    "Nous avons évalué les performances de ces modèles sur un jeu de validation, puis sélectionné le meilleur modèle pour l'évaluer sur le jeu de test. Nous avons également analysé l'importance des caractéristiques pour comprendre quels facteurs influencent le plus la satisfaction des passagers.\n",
    "\n",
    "Les techniques d'Ensemble Learning ont montré leur efficacité pour améliorer les performances de classification par rapport aux modèles individuels. Le modèle qui a obtenu les meilleures performances est le [Insérer le nom du meilleur modèle ici] avec un F1 Score de [Insérer le F1 Score ici] sur le jeu de test.\n",
    "\n",
    "Ces résultats démontrent l'intérêt des méthodes d'ensemble pour améliorer la robustesse et la précision des modèles de classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bahut_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
