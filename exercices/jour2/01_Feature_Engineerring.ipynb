{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jour 2 - Exercice 1 : Feature Engineering et Transformation des données\n",
    "\n",
    "## Objectifs\n",
    "- Comprendre l'importance du feature engineering dans le machine learning\n",
    "- Apprendre à transformer les variables catégorielles en variables numériques\n",
    "- Standardiser et normaliser les données numériques\n",
    "- Créer de nouvelles features à partir des données existantes\n",
    "- Préparer un pipeline de prétraitement pour le machine learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Le feature engineering est une étape cruciale dans tout projet de machine learning. Il s'agit de transformer les données brutes en features (caractéristiques) qui peuvent être utilisées efficacement par les algorithmes de ML. Dans ce notebook, nous allons explorer différentes techniques de feature engineering et de transformation des données en utilisant le jeu de données de satisfaction des passagers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et exploration initiale des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Pour afficher plus de colonnes dans les DataFrames\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du jeu de données\n",
    "df = pd.read_csv('../../data/passenger_satisfaction/train.csv')\n",
    "\n",
    "# Affichage des premières lignes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations sur le DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traitement des valeurs manquantes\n",
    "\n",
    "Avant de commencer le feature engineering, il est important de traiter les valeurs manquantes dans notre jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des valeurs manquantes\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "# Création d'un DataFrame pour afficher les résultats\n",
    "missing_df = pd.DataFrame({\n",
    "    'Nombre de valeurs manquantes': missing_values,\n",
    "    'Pourcentage (%)': missing_percentage\n",
    "})\n",
    "\n",
    "# Affichage des colonnes avec des valeurs manquantes\n",
    "missing_df[missing_df['Nombre de valeurs manquantes'] > 0].sort_values('Nombre de valeurs manquantes', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Visualisation des valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des valeurs manquantes avec Plotly\n",
    "missing_cols = missing_df[missing_df['Nombre de valeurs manquantes'] > 0].index.tolist()\n",
    "missing_values = missing_df.loc[missing_cols, 'Pourcentage (%)'].values\n",
    "\n",
    "fig = px.bar(\n",
    "    x=missing_cols,\n",
    "    y=missing_values,\n",
    "    labels={'x': 'Colonnes', 'y': 'Pourcentage de valeurs manquantes (%)'},\n",
    "    title='Pourcentage de valeurs manquantes par colonne',\n",
    "    color=missing_values,\n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "fig.update_layout(xaxis_tickangle=-45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Stratégie pour traiter les valeurs manquantes\n",
    "\n",
    "Nous avons plusieurs options pour traiter les valeurs manquantes :\n",
    "1. Supprimer les lignes avec des valeurs manquantes\n",
    "2. Remplacer les valeurs manquantes par une valeur par défaut (moyenne, médiane, mode, etc.)\n",
    "3. Utiliser des techniques plus avancées comme l'imputation par KNN ou par modèles prédictifs\n",
    "\n",
    "Pour cet exercice, nous allons utiliser une approche simple mais efficace :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copie du DataFrame pour ne pas modifier l'original\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Pour les variables numériques, remplacer les valeurs manquantes par la médiane\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in numeric_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "\n",
    "# Pour les variables catégorielles, remplacer les valeurs manquantes par le mode\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "\n",
    "# Vérification que toutes les valeurs manquantes ont été traitées\n",
    "print(\"Nombre total de valeurs manquantes après traitement :\", df_clean.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformation des variables catégorielles\n",
    "\n",
    "Les algorithmes de machine learning ne peuvent pas traiter directement les variables catégorielles (texte). Nous devons les convertir en variables numériques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Identification des variables catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des variables catégorielles et de leurs valeurs uniques\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nColonne: {col}\")\n",
    "    print(f\"Nombre de valeurs uniques: {df_clean[col].nunique()}\")\n",
    "    print(f\"Valeurs uniques: {df_clean[col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Encodage des variables catégorielles\n",
    "\n",
    "Il existe plusieurs méthodes pour encoder les variables catégorielles :\n",
    "1. **Label Encoding** : Attribue un nombre entier unique à chaque catégorie (0, 1, 2, etc.)\n",
    "2. **One-Hot Encoding** : Crée une nouvelle colonne binaire pour chaque catégorie\n",
    "3. **Target Encoding** : Remplace chaque catégorie par la moyenne de la variable cible pour cette catégorie\n",
    "\n",
    "Choisissons la méthode appropriée pour chaque variable :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copie du DataFrame pour les transformations\n",
    "df_encoded = df_clean.copy()\n",
    "\n",
    "# Label Encoding pour les variables ordinales ou binaires\n",
    "label_encode_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class', 'Satisfaction']\n",
    "\n",
    "for col in label_encode_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col + '_encoded'] = le.fit_transform(df_encoded[col])\n",
    "    # Afficher la correspondance entre les valeurs originales et encodées\n",
    "    mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print(f\"\\nEncodage pour {col}:\")\n",
    "    for original, encoded in mapping.items():\n",
    "        print(f\"  {original} -> {encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal Encoding pour la variable Class qui a un ordre naturel\n",
    "# Eco < Eco Plus < Business\n",
    "class_mapping = {\n",
    "    'Eco': 0,\n",
    "    'Eco Plus': 1, \n",
    "    'Business': 2\n",
    "}\n",
    "\n",
    "# Application de l'encodage ordinal\n",
    "df_encoded['Class_ordinal'] = df_encoded['Class'].map(class_mapping)\n",
    "\n",
    "# Affichage de la correspondance\n",
    "print(\"Encodage ordinal pour Class:\")\n",
    "for classe, valeur in class_mapping.items():\n",
    "    print(f\"  {classe} -> {valeur}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des premières lignes du DataFrame transformé\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Standardisation et normalisation des variables numériques\n",
    "\n",
    "De nombreux algorithmes de machine learning sont sensibles à l'échelle des variables. Il est souvent recommandé de standardiser ou normaliser les variables numériques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Identification des variables numériques à transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des colonnes numériques (en excluant l'ID et les colonnes déjà encodées)\n",
    "numeric_cols_to_scale = [col for col in numeric_cols if col != 'id' and not col.endswith('_encoded')]\n",
    "print(\"Colonnes numériques à standardiser/normaliser :\")\n",
    "print(numeric_cols_to_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Standardisation (Z-score)\n",
    "\n",
    "La standardisation transforme les données pour qu'elles aient une moyenne de 0 et un écart-type de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copie du DataFrame pour la standardisation\n",
    "df_standardized = df_encoded.copy()\n",
    "\n",
    "# Standardisation des variables numériques\n",
    "scaler = StandardScaler()\n",
    "df_standardized[numeric_cols_to_scale] = scaler.fit_transform(df_standardized[numeric_cols_to_scale])\n",
    "\n",
    "# Affichage des premières lignes des colonnes standardisées\n",
    "df_standardized[numeric_cols_to_scale].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Normalisation (Min-Max)\n",
    "\n",
    "La normalisation transforme les données pour qu'elles soient dans un intervalle spécifique, généralement [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copie du DataFrame pour la normalisation\n",
    "df_normalized = df_encoded.copy()\n",
    "\n",
    "# Normalisation des variables numériques\n",
    "normalizer = MinMaxScaler()\n",
    "df_normalized[numeric_cols_to_scale] = normalizer.fit_transform(df_normalized[numeric_cols_to_scale])\n",
    "\n",
    "# Affichage des premières lignes des colonnes normalisées\n",
    "df_normalized[numeric_cols_to_scale].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Comparaison des distributions avant et après transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection de quelques colonnes numériques pour la visualisation\n",
    "cols_to_plot = ['Age', 'Flight Distance', 'Departure Delay in Minutes']\n",
    "titles = ['Original', 'Standardisé', 'Normalisé']\n",
    "dfs = [df_clean, df_standardized, df_normalized]\n",
    "\n",
    "# Création des subplots avec plotly\n",
    "fig = make_subplots(rows=3, cols=3, \n",
    "                    subplot_titles=[f'{title}: {col}' for col in cols_to_plot for title in titles])\n",
    "\n",
    "for i, col in enumerate(cols_to_plot):\n",
    "    for j, (df, title) in enumerate(zip(dfs, titles)):\n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=df[col], name=f'{title} {col}', \n",
    "                        nbinsx=30, histnorm='probability density'),\n",
    "            row=i+1, col=j+1\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    width=1200,\n",
    "    title_text='Comparaison des distributions avant et après transformation',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Création de nouvelles features\n",
    "\n",
    "Le feature engineering ne se limite pas à transformer les variables existantes. Nous pouvons également créer de nouvelles features qui pourraient être utiles pour notre modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Création de features basées sur des connaissances du domaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copie du DataFrame pour la création de nouvelles features\n",
    "df_features = df_clean.copy()\n",
    "\n",
    "# Calcul du retard total (départ + arrivée)\n",
    "df_features['Total Delay'] = df_features['Departure Delay in Minutes'] + df_features['Arrival Delay in Minutes']\n",
    "\n",
    "# Création d'une feature indiquant si le vol a été retardé\n",
    "df_features['Is Delayed'] = (df_features['Departure Delay in Minutes'] > 0).astype(int)\n",
    "\n",
    "# Création d'une feature pour la satisfaction moyenne des services\n",
    "service_cols = [\n",
    "    'Inflight wifi service', 'Departure/Arrival time convenient', 'Ease of Online booking',\n",
    "    'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort',\n",
    "    'Inflight entertainment', 'On-board service', 'Leg room service',\n",
    "    'Baggage handling', 'Checkin service', 'Inflight service', 'Cleanliness'\n",
    "]\n",
    "df_features['Average Service Rating'] = df_features[service_cols].mean(axis=1)\n",
    "\n",
    "# Création d'une feature pour le nombre de services mal notés (note < 3)\n",
    "df_features['Poor Services Count'] = (df_features[service_cols] < 3).sum(axis=1)\n",
    "\n",
    "# Affichage des nouvelles features\n",
    "df_features[['Total Delay', 'Is Delayed', 'Average Service Rating', 'Poor Services Count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Analyse de l'importance des nouvelles features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la relation entre les nouvelles features et la satisfaction\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Retard total vs Satisfaction',\n",
    "        'Vol retardé vs Satisfaction', \n",
    "        'Note moyenne des services vs Satisfaction',\n",
    "        'Nombre de services mal notés vs Satisfaction'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Total Delay vs Satisfaction\n",
    "fig.add_trace(\n",
    "    go.Box(x=df_features['Satisfaction'], y=df_features['Total Delay'], name='Total Delay'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Is Delayed vs Satisfaction\n",
    "delayed_counts = df_features.groupby(['Is Delayed', 'Satisfaction']).size().reset_index(name='count')\n",
    "fig.add_trace(\n",
    "    go.Bar(x=delayed_counts['Is Delayed'], y=delayed_counts['count'], \n",
    "           marker_color=['#1f77b4' if x == 'satisfied' else '#ff7f0e' for x in delayed_counts['Satisfaction']], name='Is Delayed'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Average Service Rating vs Satisfaction\n",
    "fig.add_trace(\n",
    "    go.Box(x=df_features['Satisfaction'], y=df_features['Average Service Rating'], \n",
    "           name='Average Service Rating'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Poor Services Count vs Satisfaction\n",
    "fig.add_trace(\n",
    "    go.Box(x=df_features['Satisfaction'], y=df_features['Poor Services Count'], \n",
    "           name='Poor Services Count'),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text='Relation entre les nouvelles features et la satisfaction',\n",
    "    height=800,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Création d'un pipeline de prétraitement\n",
    "\n",
    "Pour automatiser le processus de prétraitement des données, nous pouvons créer un pipeline avec scikit-learn. Cela nous permettra d'appliquer facilement les mêmes transformations à de nouvelles données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des colonnes pour chaque type de transformation\n",
    "categorical_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class']\n",
    "numerical_cols = ['Age', 'Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\n",
    "service_cols = [\n",
    "    'Inflight wifi service', 'Departure/Arrival time convenient', 'Ease of Online booking',\n",
    "    'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort',\n",
    "    'Inflight entertainment', 'On-board service', 'Leg room service',\n",
    "    'Baggage handling', 'Checkin service', 'Inflight service', 'Cleanliness'\n",
    "]\n",
    "\n",
    "# Création du pipeline de prétraitement\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_cols),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_cols),\n",
    "        ('serv', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), service_cols)\n",
    "    ],\n",
    "    remainder='drop'  # Ignorer les colonnes qui ne sont pas spécifiées\n",
    ")\n",
    "\n",
    "# Affichage du pipeline\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du pipeline sur les données\n",
    "X = df.drop(['ID', 'Satisfaction'], axis=1)  # Features\n",
    "y = df['Satisfaction']  # Variable cible\n",
    "\n",
    "# Transformation des données\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Affichage de la forme des données transformées\n",
    "print(f\"Forme des données transformées: {X_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercices pratiques\n",
    "\n",
    "Maintenant que nous avons exploré différentes techniques de feature engineering, mettons en pratique ces connaissances avec quelques exercices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1: Création de nouvelles features\n",
    "\n",
    "Créez au moins deux nouvelles features qui pourraient être utiles pour prédire la satisfaction des passagers. Justifiez votre choix et analysez leur relation avec la variable cible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "# Exemple:\n",
    "df_ex1 = df_clean.copy()\n",
    "\n",
    "# Feature 1: Ratio entre la distance du vol et le retard total\n",
    "# Justification: Un petit retard sur un vol court peut être plus frustrant qu'un retard similaire sur un vol long\n",
    "df_ex1['Delay_Distance_Ratio'] = (df_ex1['Departure Delay in Minutes'] + df_ex1['Arrival Delay in Minutes']) / (df_ex1['Flight Distance'] + 1)  # +1 pour éviter division par zéro\n",
    "\n",
    "# Feature 2: Score de confort global (combinaison de plusieurs critères liés au confort)\n",
    "# Justification: Le confort global peut être un facteur déterminant de la satisfaction\n",
    "comfort_cols = ['Seat comfort', 'Leg room service', 'Cleanliness', 'Food and drink', 'Inflight entertainment']\n",
    "df_ex1['Comfort_Score'] = df_ex1[comfort_cols].mean(axis=1)\n",
    "\n",
    "# Création des box plots avec plotly\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Ratio Retard/Distance vs Satisfaction', \n",
    "                                                   'Score de confort vs Satisfaction'))\n",
    "\n",
    "# Delay_Distance_Ratio vs Satisfaction\n",
    "fig.add_trace(\n",
    "    go.Box(x=df_ex1['Satisfaction'], y=df_ex1['Delay_Distance_Ratio'], name='Delay/Distance'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Comfort_Score vs Satisfaction\n",
    "fig.add_trace(\n",
    "    go.Box(x=df_ex1['Satisfaction'], y=df_ex1['Comfort_Score'], name='Comfort'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Mise à jour du layout\n",
    "fig.update_layout(\n",
    "    title_text='Relation entre les nouvelles features et la satisfaction',\n",
    "    showlegend=False,\n",
    "    height=600,\n",
    "    width=1200\n",
    ")\n",
    "\n",
    "# Échelle logarithmique pour le premier graphique\n",
    "fig.update_yaxes(type=\"log\", row=1, col=1)\n",
    "\n",
    "# Affichage du graphique\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2: Comparaison des méthodes d'encodage\n",
    "\n",
    "Comparez l'impact de différentes méthodes d'encodage (Label Encoding vs One-Hot Encoding) sur une variable catégorielle de votre choix. Discutez des avantages et inconvénients de chaque méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "# Exemple avec la variable 'Class'\n",
    "df_ex2 = df_clean.copy()\n",
    "\n",
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "df_ex2['Class_Label_Encoded'] = le.fit_transform(df_ex2['Class'])\n",
    "print(\"Label Encoding pour 'Class':\")\n",
    "for original, encoded in zip(le.classes_, le.transform(le.classes_)):\n",
    "    print(f\"  {original} -> {encoded}\")\n",
    "\n",
    "# One-Hot Encoding\n",
    "one_hot = pd.get_dummies(df_ex2['Class'], prefix='Class')\n",
    "df_ex2 = pd.concat([df_ex2, one_hot], axis=1)\n",
    "print(\"\\nOne-Hot Encoding pour 'Class':\")\n",
    "print(one_hot.head())\n",
    "\n",
    "# Visualisation avec plotly\n",
    "fig = make_subplots(rows=1, cols=2, \n",
    "                    subplot_titles=('Label Encoding', 'One-Hot Encoding'),\n",
    "                    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}]])\n",
    "\n",
    "# Distribution de la variable avec Label Encoding\n",
    "label_counts = df_ex2['Class_Label_Encoded'].value_counts().sort_index()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=label_counts.index, y=label_counts.values, name='Label Encoding'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Distribution de la variable avec One-Hot Encoding\n",
    "fig.add_trace(\n",
    "    go.Bar(x=one_hot.columns, y=one_hot.sum(), name='One-Hot Encoding'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Mise à jour du layout\n",
    "fig.update_layout(\n",
    "    title_text='Comparaison des méthodes d\\'encodage pour la variable Class',\n",
    "    showlegend=False,\n",
    "    height=600,\n",
    "    width=1200\n",
    ")\n",
    "\n",
    "# Mise à jour des axes\n",
    "fig.update_xaxes(title_text='Class (encodée)', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Class (one-hot)', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Count', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Count', row=1, col=2)\n",
    "\n",
    "# Affichage du graphique\n",
    "fig.show()\n",
    "\n",
    "# Discussion\n",
    "print(\"\\nDiscussion:\")\n",
    "print(\"Label Encoding:\")\n",
    "print(\"  Avantages: Simple, conserve une seule colonne, peut capturer l'ordre si la variable est ordinale\")\n",
    "print(\"  Inconvénients: Introduit une relation d'ordre artificielle pour les variables nominales\")\n",
    "print(\"\\nOne-Hot Encoding:\")\n",
    "print(\"  Avantages: Pas de relation d'ordre artificielle, meilleur pour les variables nominales\")\n",
    "print(\"  Inconvénients: Augmente le nombre de colonnes, peut créer de la multicolinéarité\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3: Création d'un pipeline personnalisé\n",
    "\n",
    "Créez un pipeline personnalisé qui combine plusieurs étapes de prétraitement, y compris la création de nouvelles features, l'encodage des variables catégorielles et la standardisation des variables numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre code ici\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Création d'un transformateur personnalisé pour la création de features\n",
    "class FeatureCreator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Calcul du retard total\n",
    "        X_copy['Total_Delay'] = X_copy['Departure Delay in Minutes'] + X_copy['Arrival Delay in Minutes']\n",
    "        \n",
    "        # Score de confort\n",
    "        comfort_cols = ['Seat comfort', 'Leg room service', 'Cleanliness', 'Food and drink', 'Inflight entertainment']\n",
    "        X_copy['Comfort_Score'] = X_copy[comfort_cols].mean(axis=1)\n",
    "        \n",
    "        # Score de service\n",
    "        service_cols = ['Inflight wifi service', 'On-board service', 'Baggage handling', 'Checkin service', 'Inflight service']\n",
    "        X_copy['Service_Score'] = X_copy[service_cols].mean(axis=1)\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "# Création du pipeline personnalisé\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Définition des colonnes\n",
    "categorical_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class']\n",
    "numerical_cols = ['Age', 'Flight Distance', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\n",
    "service_cols = [\n",
    "    'Inflight wifi service', 'Departure/Arrival time convenient', 'Ease of Online booking',\n",
    "    'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort',\n",
    "    'Inflight entertainment', 'On-board service', 'Leg room service',\n",
    "    'Baggage handling', 'Checkin service', 'Inflight service', 'Cleanliness'\n",
    "]\n",
    "\n",
    "# Colonnes à conserver pour la création de features\n",
    "cols_to_keep = numerical_cols + categorical_cols + service_cols\n",
    "\n",
    "# Pipeline complet\n",
    "custom_pipeline = Pipeline([\n",
    "    ('feature_creator', FeatureCreator()),\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numerical_cols + ['Total_Delay', 'Comfort_Score', 'Service_Score']),\n",
    "            ('cat', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "            ]), categorical_cols),\n",
    "            ('serv', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), service_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Test du pipeline\n",
    "X = df.drop(['ID', 'Satisfaction'], axis=1)  # Features\n",
    "y = df['Satisfaction']  # Variable cible\n",
    "\n",
    "# Application du pipeline\n",
    "X_transformed = custom_pipeline.fit_transform(X)\n",
    "print(f\"Forme des données transformées avec le pipeline personnalisé: {X_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons exploré différentes techniques de feature engineering et de transformation des données :\n",
    "\n",
    "1. **Traitement des valeurs manquantes** : Nous avons identifié et traité les valeurs manquantes dans notre jeu de données.\n",
    "2. **Transformation des variables catégorielles** : Nous avons utilisé le Label Encoding et le One-Hot Encoding pour convertir les variables catégorielles en variables numériques.\n",
    "3. **Standardisation et normalisation** : Nous avons appliqué différentes techniques pour mettre à l'échelle les variables numériques.\n",
    "4. **Création de nouvelles features** : Nous avons créé de nouvelles features à partir des données existantes pour améliorer la performance des modèles.\n",
    "5. **Création d'un pipeline de prétraitement** : Nous avons automatisé le processus de prétraitement des données avec un pipeline scikit-learn.\n",
    "\n",
    "Ces techniques sont essentielles pour préparer les données avant d'entraîner des modèles de machine learning. Dans le prochain notebook, nous utiliserons ces données prétraitées pour entraîner nos premiers modèles de classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bahut_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
